{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "FHjHBLybhoYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!mkdir saved_models\n",
        "!mkdir saved_images"
      ],
      "metadata": {
        "id": "TdQvjsFNOk4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdJkN1ua3mW5"
      },
      "outputs": [],
      "source": [
        "!pip install chaospy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ZtnqG93i18"
      },
      "source": [
        "Notes:\n",
        "\n",
        "the loss outputs a row vector --- the NN model outputs a column vector\n",
        "\n",
        "data: batch x row vector  ->  [raio, comprimento, tempo, peclet number, knudsen number * beta_v, brinkman number]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D5Hm843iKXpb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math \n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.autograd.functional as F\n",
        "import torch.utils.tensorboard as board\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import mpl_toolkits.mplot3d as plt_3d\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import chaospy\n",
        "import scipy\n",
        "import sklearn as sk\n",
        "from scipy.interpolate import interp2d\n",
        "from sklearn import mixture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoYydNPG3i1_"
      },
      "source": [
        "Cleaning CUDA memory "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RtIp3GG3i1_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bO2a5Wg3i2A"
      },
      "source": [
        "miscellaneous functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S-LBBpeg3i2A"
      },
      "outputs": [],
      "source": [
        "# transform column vector into a row vector\n",
        "def column_to_row(x):\n",
        "    return x.reshape(len(x))\n",
        "\n",
        "# stack the data\n",
        "def especial_sorting(data, axis=0, engine=\"numpy\"):   # data shape =>  [batch X domain / EX: [20000x4]]\n",
        "    if engine == \"numpy\":\n",
        "        data_sorted = np.zeros(data.shape)\n",
        "        for idx, i in enumerate(np.argsort(data.T[axis])):\n",
        "            data_sorted[idx] = data[i]\n",
        "        return data_sorted\n",
        "    if engine == \"torch\":\n",
        "        data_sorted = torch.zeros_like(data)\n",
        "        for idx, i in enumerate(torch.argsort(data.T[axis])):\n",
        "            data_sorted[idx] = data[i]\n",
        "        return data_sorted\n",
        "\n",
        "# collect error\n",
        "def collect_error(loss, data, percentage=0.1):\n",
        "    \n",
        "    num_data_erro = int(len(loss)*percentage)\n",
        "    data_erro = np.hstack((loss, data))\n",
        "\n",
        "    data_erro = especial_sorting(data_erro)\n",
        "    \n",
        "    return data_erro[:num_data_erro,1:]\n",
        "\n",
        "# split the data (use for separate the data into train and test)\n",
        "def split_data(data, percent):\n",
        "    return torch.split(data,[math.floor(len(data)*percent),len(data)-math.floor(len(data)*percent)])\n",
        "\n",
        "# reshape_data (use for gradient accumulation)\n",
        "def reshape_data(data, ac_iter):\n",
        "    if (len(data)%ac_iter) == 0:\n",
        "        return torch.reshape(data,(ac_iter,int(len(data)/ac_iter),int(len(data[0]))))\n",
        "\n",
        "    if (len(data)%ac_iter) != 0:\n",
        "        Y = (len(data)%ac_iter)\n",
        "        return torch.reshape(data[:-Y],(ac_iter,int(len(data[:-Y])/ac_iter),int(len(data[0]))))\n",
        "\n",
        "# load the NN model parameters\n",
        "def load_params(model,PATH,cpu_only=False):\n",
        "    if cpu_only is True:\n",
        "        return model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu')))\n",
        "    if cpu_only is False:\n",
        "        return model.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW0Iohtz3i2B"
      },
      "source": [
        "--------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0UWKjGi3i2B"
      },
      "source": [
        "Código de treinamento PINN para o TCC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s_77BsZKsVH"
      },
      "source": [
        "Definição do domínio de estudo, os \"dados\" a serem treinados na rede neural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ArgZNRGfK2bI"
      },
      "outputs": [],
      "source": [
        "#-----------------sampling functions-------------------- (data exmplo: [x_menor,x_maior,y_menor,y_maior,t_menor,t_maior])\n",
        "#data sampling\n",
        "def data_sampling(data,count, rule=\"sobol\"):\n",
        "    uniform_cube = chaospy.J(chaospy.Uniform(data[0], data[1]), \n",
        "                             chaospy.Uniform(data[2], data[3]), \n",
        "                             chaospy.Uniform(data[4], data[5]),\n",
        "                             chaospy.Uniform(data[6], data[7]))\n",
        "    sobol_samples = uniform_cube.sample(count, rule)\n",
        "    return sobol_samples\n",
        "\n",
        "#mapping to a 2D line (use lambda t: F(t)) - \"chebyshev\" / \"korobov\"  (row vector: [x,y])\n",
        "def smap(func,count,data,v_dependente,metodo):\n",
        "    if v_dependente == \"x\":\n",
        "        uniform_line = chaospy.J(chaospy.Uniform(data[2], data[3]), \n",
        "                                 chaospy.Uniform(data[4], data[5]),\n",
        "                                 chaospy.Uniform(data[6], data[7]))\n",
        "        samples = uniform_line.sample(count, rule=metodo)\n",
        "        transform_x = np.vstack((func(samples[0]),samples))\n",
        "        return transform_x\n",
        "    \n",
        "    if v_dependente == \"y\":\n",
        "        uniform_line = chaospy.J(chaospy.Uniform(data[0], data[1]), \n",
        "                                 chaospy.Uniform(data[4], data[5]),\n",
        "                                 chaospy.Uniform(data[6], data[7]))\n",
        "        samples = uniform_line.sample(count, rule=metodo)\n",
        "        transform_y = np.vstack((samples[0],func(samples[0]),samples[1:]))\n",
        "        return transform_y\n",
        "\n",
        "    if v_dependente == \"t\":\n",
        "        uniform_line = chaospy.J(chaospy.Uniform(data[0], data[1]), \n",
        "                                 chaospy.Uniform(data[2], data[3]),\n",
        "                                 chaospy.Uniform(data[6], data[7]))\n",
        "        samples = uniform_line.sample(count, rule=metodo)\n",
        "        transform_t = np.vstack((samples[0],samples[1],func(samples[1]),samples[2:]))\n",
        "        return transform_t\n",
        "\n",
        "#adaptive sampling (data -> [batch,dimension])\n",
        "def adaptive_sampling(data, num_samples, num_modes=1, tolerance=0.001):\n",
        "    \n",
        "    model = mixture.GaussianMixture(num_modes,tol=tolerance)\n",
        "    model.fit(data)\n",
        "\n",
        "    D = chaospy.GaussianMixture(model.means_, model.covariances_)\n",
        "\n",
        "    return D.sample(num_samples,rule=\"sobol\")\n",
        "\n",
        "#specify the domain (use lambda t: F(t))\n",
        "def S_domain(data_domain,func_S,func_I,v_menor,v_maior):\n",
        "    for i in range(len(data_domain[0])):\n",
        "        if data_domain[0,i]>v_maior or data_domain[0,i]<v_menor:\n",
        "            continue\n",
        "        if func_S==func_I:\n",
        "            data_domain[0,i]=math.nan\n",
        "            continue\n",
        "        if data_domain[1,i]>func_S(data_domain[0,i]) or data_domain[1,i]<func_I(data_domain[0,i]):\n",
        "            data_domain[0,i]=math.nan\n",
        "    data_domain=data_domain[:, ~np.isnan(data_domain).any(axis=0)]\n",
        "    return data_domain\n",
        "\n",
        "def ST_domain(data_domain, dim, menor, maior):\n",
        "    for i in range(len(data_domain[0])):\n",
        "        if data_domain[dim,i]>maior or data_domain[dim,i]<menor:\n",
        "            data_domain[dim,i]=math.nan\n",
        "    data_domain=data_domain[:, ~np.isnan(data_domain).any(axis=0)]\n",
        "    return data_domain\n",
        "\n",
        "#-----------------sampling the domain-------------------- \n",
        "total_num_samples = 120000\n",
        "dy_num_samples = 25000\n",
        "problem_domain = [0,1,0,12,0,10,0.0002,1]\n",
        "\n",
        "uniform_samples = data_sampling(problem_domain, total_num_samples).T\n",
        "dynamic_S_CPU, constant_S_CPU = uniform_samples[:dy_num_samples],uniform_samples[dy_num_samples:]\n",
        "\n",
        "#-----------------sampling the boundary-------------------- \n",
        "CC_1_CPU=smap(lambda t: t*0,20000,problem_domain,\"x\",\"sobol\").T\n",
        "CC_2_CPU=smap(lambda t: t*0+1,20000,problem_domain,\"x\",\"sobol\").T\n",
        "CC_3_CPU=smap(lambda t: t*0,20000,problem_domain,\"y\",\"sobol\").T\n",
        "CC_4_CPU=smap(lambda t: t*0+12,20000,problem_domain,\"y\",\"sobol\").T\n",
        "CC_5_CPU=smap(lambda x: x*0,20000,problem_domain,\"t\",\"sobol\").T\n",
        "\n",
        "#-----------------converting to tensor--------------------\n",
        "dynamic_S = torch.from_numpy(dynamic_S_CPU)\n",
        "constant_S = torch.from_numpy(constant_S_CPU)\n",
        "CC_1 = torch.from_numpy(CC_1_CPU)\n",
        "CC_2 = torch.from_numpy(CC_2_CPU)\n",
        "CC_3 = torch.from_numpy(CC_3_CPU)\n",
        "CC_4 = torch.from_numpy(CC_4_CPU)\n",
        "CC_5 = torch.from_numpy(CC_5_CPU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TMCA data"
      ],
      "metadata": {
        "id": "_HtxZb_aJpvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TMAC_data = pd.read_excel('/content/Tangential_Momentum_Accommodation_Coefficient_Data.xlsx').values.T[0]\n",
        "# TMAC_data += np.random.uniform(-0.0005,0.0005,len(TMAC_data))\n",
        "# TMAC_data = chaospy.GaussianKDE(TMAC_data, h_mat=0.008).sample(3000,rule=\"latin_hypercube\")\n",
        "# for i in range(len(TMAC_data)):\n",
        "#     if TMAC_data[i]<0.1:\n",
        "#         TMAC_data[i]=np.mean(TMAC_data)\n",
        "\n",
        "# beta_v = (2-TMAC_data)/TMAC_data"
      ],
      "metadata": {
        "id": "n-rc25n-JpJp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXpBA6L3i2E"
      },
      "source": [
        "Definição das equações a serem estudadas (problem definition) e condições de contorno / iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kkqIcf5c3i2E"
      },
      "outputs": [],
      "source": [
        "b = 2.03704\n",
        "\n",
        "#---------------------PDE------------------------\n",
        "\n",
        "def PDE_loss(NN,data):\n",
        "    \n",
        "    grad = torch.autograd.grad(NN.sum(),data,create_graph=True,retain_graph=True)[0]\n",
        "\n",
        "    T_r = grad[:,0]\n",
        "    T_z = grad[:,1]\n",
        "    T_t = grad[:,2]\n",
        "\n",
        "    T_rr = (torch.autograd.grad(T_r.sum(),data,create_graph=False,retain_graph=True)[0])[:,0]\n",
        "    T_zz = (torch.autograd.grad(T_z.sum(),data,create_graph=False,retain_graph=True)[0])[:,1]\n",
        "    \n",
        "    pe = 1\n",
        "\n",
        "    return ((data[:,0]*(pe)*(0.5+(4.0*data[:,3]))*(T_t-T_rr))+\n",
        "            (data[:,0]*(pe)*(1+(4.0*data[:,3])-(data[:,0]**2))*T_z)-\n",
        "            ((0.5+(4.0*data[:,3]))*(((pe)*T_r)+(data[:,0]*T_zz))))\n",
        "\n",
        "#---------------------boundary and initial conditions------------------------\n",
        "def BC_1(NN,bc):\n",
        "    \n",
        "    T = torch.autograd.grad(NN.sum(),bc,create_graph=False,retain_graph=True)[0]\n",
        "    \n",
        "    return T[:,0]\n",
        "\n",
        "\n",
        "def BC_2(NN,bc):\n",
        "    \n",
        "    T = torch.autograd.grad(NN.sum(),bc,create_graph=False,retain_graph=True)[0]\n",
        "    \n",
        "    return column_to_row(NN)+(b*bc[:,3]*T[:,0])-1\n",
        "    \n",
        "def BC_3(NN):\n",
        "    \n",
        "    return column_to_row(NN)\n",
        "    \n",
        "def BC_4(NN,bc):\n",
        "    \n",
        "    T = torch.autograd.grad(NN.sum(),bc,create_graph=False,retain_graph=True)[0]\n",
        "\n",
        "    return T[:,1]\n",
        "\n",
        "def BC_5(NN):\n",
        "    \n",
        "    return column_to_row(NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAGsnMRPLh4Z"
      },
      "source": [
        "Definição da estrutura da rede neural "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9s9Kx2FELs2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df89992-83f1-4c8b-f50d-c477109cd1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (input): Linear(in_features=4, out_features=100, bias=True)\n",
            "  (input_actv): Stan(\n",
            "    (tan_h): Tanh()\n",
            "  )\n",
            "  (mid_sequential): Sequential(\n",
            "    (0): Block(\n",
            "      (FF_layer_1): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (FF_layer_2): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (actv): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "      (actv_2): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (FF_layer_1): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (FF_layer_2): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (actv): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "      (actv_2): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (2): Block(\n",
            "      (FF_layer_1): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (FF_layer_2): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (actv): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "      (actv_2): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (3): Block(\n",
            "      (FF_layer_1): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (FF_layer_2): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (actv): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "      (actv_2): Stan(\n",
            "        (tan_h): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "writer = board.SummaryWriter('runs/Código_PINN_microtubulações_6')\n",
        "\n",
        "# Define the Stan activation function\n",
        "class Stan(nn.Module):\n",
        "    def __init__(self,dim_act):\n",
        "        super(Stan, self).__init__()\n",
        "        \n",
        "        self.tan_h = nn.Tanh()\n",
        "\n",
        "        shape = (dim_act,)\n",
        "        self.Learnable_P = nn.Parameter(torch.Tensor(*shape))\n",
        "        self.init_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tan_h(x) + (self.Learnable_P*x*self.tan_h(x))\n",
        "\n",
        "    def init_parameters(self):\n",
        "        nn.init.uniform_(self.Learnable_P)\n",
        "\n",
        "# Define the block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,dim):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        self.FF_layer_1 = nn.Linear(dim, dim)\n",
        "        self.FF_layer_2 = nn.Linear(dim, dim)\n",
        "        \n",
        "        # self.actv = nn.SiLU()\n",
        "        # self.actv_2 = nn.SiLU()\n",
        "        self.actv = Stan(dim)\n",
        "        self.actv_2 = Stan(dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        identity = X.clone()\n",
        "        \n",
        "        X = self.FF_layer_1(X)\n",
        "        X = self.actv(X)\n",
        "        X = self.FF_layer_2(X)\n",
        "\n",
        "        X += identity\n",
        "        X = self.actv_2(X)\n",
        "        return X\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        \n",
        "        input_dim = 4\n",
        "        mid_dim = 100\n",
        "        out_dim = 1\n",
        "        num_of_blocks = 4\n",
        "        \n",
        "        \n",
        "        self.input = nn.Linear(input_dim, mid_dim)\n",
        "        # self.input_actv = nn.SiLU()\n",
        "        self.input_actv = Stan(mid_dim)\n",
        "        \n",
        "        self.mid_layers_list = []\n",
        "        for _ in range(num_of_blocks):\n",
        "            self.mid_layers_list.append(Block(mid_dim))\n",
        "        self.mid_sequential = nn.Sequential(*self.mid_layers_list)\n",
        "        \n",
        "        self.output = nn.Linear(mid_dim, out_dim)\n",
        "\n",
        "        self.input.apply(self.init_weights)\n",
        "        self.mid_sequential.apply(self.init_weights)\n",
        "        self.output.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        Z = self.input(x)\n",
        "        Z = self.input_actv(Z)\n",
        "        Z = self.mid_sequential(Z)\n",
        "        Z = self.output(Z)\n",
        "        \n",
        "        return Z\n",
        "\n",
        "    def init_weights(self,m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i3j3_d3LtVd"
      },
      "source": [
        "Definição do solver(otimização)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ouoW7RmXL6iP"
      },
      "outputs": [],
      "source": [
        "# --------- Training parameters ------------\n",
        "def Causal_MSELoss(X,causal_res):\n",
        "    return torch.mean(causal_res*(torch.pow((X),2)))\n",
        "\n",
        "def Adap_MSELoss(X):\n",
        "    return torch.mean(torch.pow(X,2))\n",
        "\n",
        "loss_fn_adap = Adap_MSELoss\n",
        "loss_fn_causal = Causal_MSELoss\n",
        "loss_fnc = nn.MSELoss()\n",
        "\n",
        "optimizer_adam = torch.optim.Adam(model.parameters(), lr=8.0e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_adam, milestones=[10000, 20000, 40000], gamma=0.5)\n",
        "\n",
        "# --------- Training function -----------\n",
        "def train(X_,CC_1_,CC_2_,CC_3_,CC_4_,CC_5_, model, loss_fn, loss_fn_B, optimizer, i, accum_iter=1, causal_p=300):\n",
        "    loss_board = 0\n",
        "    model.train()\n",
        "    \n",
        "    X_ = reshape_data(X_, accum_iter)\n",
        "    CC_1_ = reshape_data(CC_1_, accum_iter)\n",
        "    CC_2_ = reshape_data(CC_2_, accum_iter)\n",
        "    CC_3_ = reshape_data(CC_3_, accum_iter)\n",
        "    CC_4_ = reshape_data(CC_4_, accum_iter)\n",
        "    CC_5_ = reshape_data(CC_5_, accum_iter)\n",
        "    \n",
        "    for idx in range(accum_iter):\n",
        "        \n",
        "        X = X_[idx].to(device,dtype=torch.float32).requires_grad_()\n",
        "        CC_1 = CC_1_[idx].to(device,dtype=torch.float32).requires_grad_()\n",
        "        CC_2 = CC_2_[idx].to(device,dtype=torch.float32).requires_grad_()\n",
        "        CC_3 = CC_3_[idx].to(device,dtype=torch.float32).requires_grad_()\n",
        "        CC_4 = CC_4_[idx].to(device,dtype=torch.float32).requires_grad_()\n",
        "        CC_5 = CC_5_[idx].to(device,dtype=torch.float32).requires_grad_()\n",
        "        # Y = torch.zeros(len(X),device=device,dtype=torch.float32).requires_grad_()\n",
        "        # Y_bc = torch.zeros(len(CC_1)+len(CC_2)+len(CC_3)+len(CC_4)+len(CC_5),device=device,dtype=torch.float32).requires_grad_()\n",
        "\n",
        "        time = X_[idx,:,2].to(device,dtype=torch.float32)\n",
        "        \n",
        "        def computing_loss_train():\n",
        "            pred_domain = PDE_loss(model(X),X)\n",
        "            pred_boundery_1 = BC_1(model(CC_1),CC_1)\n",
        "            pred_boundery_2 = BC_2(model(CC_2),CC_2)\n",
        "            pred_boundery_3 = BC_3(model(CC_3))\n",
        "            pred_boundery_4 = BC_4(model(CC_4),CC_4)\n",
        "            pred_boundery_5 = BC_5(model(CC_5))\n",
        "            # pred_boundery = torch.hstack((pred_boundery_1,pred_boundery_2,pred_boundery_3,pred_boundery_4,pred_boundery_5))\n",
        "            \n",
        "            iteracao = 0\n",
        "            causal_W = torch.exp(-time*(causal_p/(causal_p+i+iteracao)))\n",
        "            # causal_W = 1\n",
        "\n",
        "            return (loss_fn(pred_domain,causal_W)+\n",
        "                    5*loss_fn_B(pred_boundery_1)+\n",
        "                    5*loss_fn_B(pred_boundery_2)+\n",
        "                    5*loss_fn_B(pred_boundery_3)+\n",
        "                    2*loss_fn_B(pred_boundery_4)+\n",
        "                    2*loss_fn_B(pred_boundery_5))\n",
        "\n",
        "        # Compute prediction error\n",
        "        loss = computing_loss_train()/accum_iter\n",
        "        loss_board += loss\n",
        "            \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "            \n",
        "        if (idx+1) % accum_iter == 0: \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "            \n",
        "    writer.add_scalar('train loss', loss_board.item(), global_step=i)\n",
        "\n",
        "    # print(f\"train loss: \\n Avg loss: {loss_board.item():>8f} \\n\")\n",
        "            \n",
        "def test(X_,CC_1_,CC_2_,CC_3_,CC_4_,CC_5_, model, loss_fn, i):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    X = X_.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_1 = CC_1_.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_2 = CC_2_.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_3 = CC_3_.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_4 = CC_4_.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_5 = CC_5_.to(device,dtype=torch.float32).requires_grad_()\n",
        "    # Y = torch.zeros(len(X),device=device,dtype=torch.float32).requires_grad_()\n",
        "    # Y_bc = torch.zeros(len(CC_1)+len(CC_2)+len(CC_3)+len(CC_4)+len(CC_5),device=device,dtype=torch.float32).requires_grad_()\n",
        "    \n",
        "    pred_domain = PDE_loss(model(X),X)\n",
        "    pred_boundery_1 = BC_1(model(CC_1),CC_1)\n",
        "    pred_boundery_2 = BC_2(model(CC_2),CC_2)\n",
        "    pred_boundery_3 = BC_3(model(CC_3))\n",
        "    pred_boundery_4 = BC_4(model(CC_4),CC_4)\n",
        "    pred_boundery_5 = BC_5(model(CC_5))\n",
        "    # pred_boundery = torch.hstack((pred_boundery_1,pred_boundery_2,pred_boundery_3,pred_boundery_4,pred_boundery_5))\n",
        "    \n",
        "    domain_test = loss_fn(pred_domain)\n",
        "    boundery_test_1 = loss_fn(pred_boundery_1)\n",
        "    boundery_test_2 = loss_fn(pred_boundery_2)\n",
        "    boundery_test_3 = loss_fn(pred_boundery_3)\n",
        "    boundery_test_4 = loss_fn(pred_boundery_4)\n",
        "    boundery_test_5 = loss_fn(pred_boundery_5)\n",
        "    test_loss = domain_test+boundery_test_1+boundery_test_2+boundery_test_3+boundery_test_4+boundery_test_5\n",
        "\n",
        "    \n",
        "    # print(f\"Test Error: \\n Avg loss: {test_loss.item():>8f} \\n\")\n",
        "    writer.add_scalar('test loss', test_loss.item(), global_step=i)\n",
        "    writer.add_scalar('domain loss', domain_test.item(), global_step=i)\n",
        "    writer.add_scalar('boundery 1 loss', boundery_test_1.item(), global_step=i)\n",
        "    writer.add_scalar('boundery 2 loss', boundery_test_2.item(), global_step=i)\n",
        "    writer.add_scalar('boundery 3 loss', boundery_test_3.item(), global_step=i)\n",
        "    writer.add_scalar('boundery 4 loss', boundery_test_4.item(), global_step=i)\n",
        "    writer.add_scalar('boundery 5 loss', boundery_test_5.item(), global_step=i)\n",
        "    \n",
        "    return test_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peA1e7pPL98i"
      },
      "source": [
        "Treinamento e \"running time operations\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bpe3VpI_MDBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ab52b4-866e-4199-c433-d8e943d53d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 500000\n",
        "grad_accum = 1\n",
        "model_pre_comp, continue_comp = True ,False\n",
        "data_pre_comp = False\n",
        "\n",
        "#---------------------separete the data--------------------------\n",
        "Percent_of_train = 0.8\n",
        "\n",
        "dynamic_train ,dynamic_test = split_data(dynamic_S, Percent_of_train)\n",
        "constant_train ,constant_test = split_data(constant_S, Percent_of_train)\n",
        "CC_1_train ,CC_1_test = split_data(CC_1, Percent_of_train)\n",
        "CC_2_train ,CC_2_test = split_data(CC_2, Percent_of_train)\n",
        "CC_3_train ,CC_3_test = split_data(CC_3, Percent_of_train)\n",
        "CC_4_train ,CC_4_test = split_data(CC_4, Percent_of_train)\n",
        "CC_5_train ,CC_5_test = split_data(CC_5, Percent_of_train)\n",
        "\n",
        "len_dyn_train = len(dynamic_train)\n",
        "DS_train ,DS_test = torch.vstack((dynamic_train, constant_train)) ,torch.vstack((dynamic_test, constant_test))\n",
        "\n",
        "DS_train = especial_sorting(DS_train, axis=2, engine=\"torch\")\n",
        "\n",
        "if data_pre_comp is True:\n",
        "    DS_train = pre_comp\n",
        "\n",
        "if model_pre_comp is True:\n",
        "    load_params(model,\"saved_models/save_microtubulações.pth\",cpu_only=True)\n",
        "\n",
        "test_loss_save=100\n",
        "for t in range(epochs):\n",
        "\n",
        "    if continue_comp is not True:\n",
        "        break\n",
        "    \n",
        "    # ---------importling sampling-------------------\n",
        "\n",
        "    # if (t+1)%500 == 0 and t<8000:\n",
        "\n",
        "    #     # ---------sampling to avoid memory issue-------------------\n",
        "    #     randon_permutation = torch.randperm(len(DS_train))\n",
        "    #     import_sampl_data = torch.zeros(int(len(DS_train)/grad_accum),len(DS_train[0]))\n",
        "    #     for i in range(len(import_sampl_data)):\n",
        "    #         import_sampl_data[i] = DS_train[randon_permutation[i].item()]\n",
        "\n",
        "\n",
        "    #     dy_gpu = import_sampl_data.to(device,dtype=torch.float32).requires_grad_()\n",
        "    #     error_gpu = PDE_loss(model(dy_gpu),dy_gpu)\n",
        "    #     dy_cpu = dy_gpu.cpu().detach().numpy()\n",
        "    #     error_cpu = error_gpu.cpu().detach().numpy()\n",
        "        \n",
        "    #     adap_sam = adaptive_sampling(collect_error(error_cpu.reshape(1,len(error_cpu)).T, dy_cpu, percentage=0.2), len_dyn_train, num_modes=5)\n",
        "        \n",
        "    #     adap_sam = ST_domain(adap_sam, 0, 0, 1)\n",
        "    #     adap_sam = ST_domain(adap_sam, 1, 0, 25)\n",
        "    #     adap_sam = ST_domain(adap_sam, 2, 0, 10)\n",
        "    #     adap_sam = (ST_domain(adap_sam, 3, 0.0002,1)).T\n",
        "        \n",
        "    #     dynamic_train = torch.from_numpy(adap_sam)\n",
        "        \n",
        "    #     DS_train = torch.vstack((dynamic_train, constant_train))\n",
        "\n",
        "    #     DS_train = especial_sorting(DS_train, axis=2, engine=\"torch\")\n",
        "    \n",
        "    #--------------------------------------------------------\n",
        "    \n",
        "    train(DS_train, CC_1_train, CC_2_train, CC_3_train, CC_4_train, CC_5_train, model, loss_fn_causal, loss_fn_adap, optimizer_adam, t, accum_iter=grad_accum)\n",
        "    test_loss_epochs = test(DS_test, CC_1_test, CC_2_test, CC_3_test, CC_4_test, CC_5_test, model, loss_fn_adap, t)\n",
        "        \n",
        "    #--------------------------------------------------------\n",
        "    \n",
        "    if t%30 == 0:\n",
        "        if test_loss_epochs<test_loss_save:\n",
        "            test_loss_save=test_loss_epochs\n",
        "            iter_save_model = t\n",
        "            torch.save(model.state_dict(), \"saved_models/save_microtubulações.pth\")\n",
        "\n",
        "writer.close()\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEINueiWMKmh"
      },
      "source": [
        "funções de Pós-processamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms the NN output data in grid form for ploting\n",
        "def data_for_ploting(model, dim, Y_dim, pde_error=None, time_domain=None, others_domains=None):\n",
        "    ploting_dataY, ploting_dataX = np.mgrid[Y_dim[0]:Y_dim[1]:dim*1j,0:1:dim*1j]\n",
        "    \n",
        "    # transform the grid data into batches ([number_of_points, 2])\n",
        "    X = np.hstack((np.swapaxes([ploting_dataX.flatten()],0,1),np.swapaxes([ploting_dataY.flatten()],0,1)))\n",
        "    X = torch.from_numpy(X)\n",
        "    if time_domain is not None:\n",
        "        X = torch.hstack((X,torch.ones(len(X), 1)*time_domain))\n",
        "    if others_domains is not None:\n",
        "        for ii in range(len(others_domains)):\n",
        "            X = torch.hstack((X,torch.ones(len(X), 1)*others_domains[ii]))\n",
        "            \n",
        "    model.eval()\n",
        "    X = X.to(device,dtype=torch.float32).requires_grad_()\n",
        "    Y = model(X)\n",
        "    if pde_error is not None:\n",
        "        Y = pde_error(Y,X)\n",
        "    Y = Y.cpu()\n",
        "    Y = Y.detach().numpy()\n",
        "    \n",
        "    grid_form = np.reshape(Y,(dim, dim))\n",
        "    \n",
        "    return grid_form\n",
        "\n",
        "# mirroring the data\n",
        "def symmetric_data(Z):\n",
        "    Z_sym = Z[:,0].reshape(Z.shape[0],1)\n",
        "    for i in range(Z.shape[1]-1):\n",
        "        Z_sym = np.hstack((Z[:,i+1].reshape(Z.shape[0],1),Z_sym))\n",
        "    return np.hstack((Z_sym,Z))\n",
        "\n",
        "def symmetric_data_1D(Z):\n",
        "    Z_sym = np.array([Z[0]])\n",
        "    for i in range(len(Z)-1):\n",
        "        Z_sym = np.hstack((-Z[i+1],Z_sym))\n",
        "    return np.hstack((Z_sym,Z))"
      ],
      "metadata": {
        "id": "25vGUnS9Bz_U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pós-processamento dos dados"
      ],
      "metadata": {
        "id": "mNb9_VxKB14Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwqhsE-K3i2H"
      },
      "outputs": [],
      "source": [
        "pixel_y,pixel_x=1500,1500                   #   <<<<<<------       <<<<<<------         <<<<<<------\n",
        "\n",
        "#--------------domain----------------\n",
        "\n",
        "# make data with even sampling in x\n",
        "plot_dim = 400                                            #   <<<<<<------       <<<<<<------         <<<<<<------\n",
        "\n",
        "X_grid = np.linspace(-1, 1, num=plot_dim*2)\n",
        "Y_grid = np.linspace(0, 3, num=plot_dim)\n",
        "\n",
        "Z = data_for_ploting(model, plot_dim, pde_error=None, time_domain=19, others_domains=[1])\n",
        "Z = symmetric_data(Z)\n",
        "\n",
        "#------------- interpolating the data ---------------------\n",
        "f = interp2d(X_grid, Y_grid, Z, kind='cubic')\n",
        "X_interp = np.linspace(-1, 1, num=1500)\n",
        "Y_interp = np.linspace(0, 3, num=1500)\n",
        "interp_data = f(X_interp,Y_interp)\n",
        "\n",
        "#--------------Image creation----------------\n",
        "\n",
        "px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
        "fig, ax = plt.subplots(figsize=(pixel_x*px, pixel_y*px))\n",
        "\n",
        "#--------------plots----------------\n",
        "\n",
        "plot_mapping = ax.pcolormesh(X_interp, Y_interp, interp_data,antialiased=True,cmap='hot')\n",
        "\n",
        "fig.colorbar(plot_mapping)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcmOTm0A3i2H"
      },
      "outputs": [],
      "source": [
        "def plot_3D_K(t, domains, iteracao=None):\n",
        "    #--------------Image creation----------------\n",
        "\n",
        "    pixel_y,pixel_x=1500,1500\n",
        "    plot_dim_3d = 80\n",
        "    Y_dim = [0.03,1]\n",
        "\n",
        "    px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
        "\n",
        "    #--------------data for the plots----------------\n",
        "\n",
        "    Z, RAIO = np.mgrid[Y_dim[0]:Y_dim[1]:plot_dim_3d*1j,-1:1:(plot_dim_3d*2)*1j]\n",
        "    \n",
        "    temp = data_for_ploting(model, plot_dim_3d, Y_dim, pde_error=None, time_domain=t, others_domains=domains)\n",
        "    temp = symmetric_data(temp)\n",
        "    \n",
        "    #----------------- Matplotlib -------------------\n",
        "\n",
        "    fig_3d, ax_3d = plt.subplots(figsize=(15, 8))\n",
        "    ax_3d = plt.axes(projection='3d')\n",
        "    \n",
        "    ax_3d.plot_surface(RAIO, Z, temp, rstride=1, cstride=1, cmap='viridis')\n",
        "    # ax_3d.plot_wireframe(RAIO, Z, temp, color='black')\n",
        "    ax_3d.set_xlabel('\\n \\n \\n \\n \\n raio', rotation=45)\n",
        "    ax_3d.set_ylabel('\\n \\n comprimento')\n",
        "    ax_3d.set_zlabel('\\n \\n temperatura')\n",
        "    ax_3d.set_title(f\"Distribuição de temperatura \\n Kn Bv = {2*domains[0]} e Br = 0 tempo = {t} \\n Figura: d) \\n\")\n",
        "    ax_3d.set_zlim(0,1.1)\n",
        "    plt.xticks(rotation = 45)\n",
        "\n",
        "    for item in ([ax_3d.title, ax_3d.xaxis.label, ax_3d.yaxis.label,ax_3d.zaxis.label] +\n",
        "             ax_3d.get_xticklabels()+ax_3d.get_yticklabels()+ax_3d.get_zticklabels()):\n",
        "        item.set_fontsize(20)\n",
        "    \n",
        "    # ax_3d.view_init(35, 50);\n",
        "    if iteracao is None:\n",
        "        fig_3d.savefig(f\"saved_images/kn_bv={2*domains[0]},Br=0,tempo={t}.svg\",format=\"svg\", dpi=60)\n",
        "    else:\n",
        "        fig_3d.savefig(f\"saved_images_3/{iteracao}.svg\",format=\"svg\", dpi=60)\n",
        "\n",
        "    #----------------- plotly ----------------------\n",
        "    # fig = go.Figure(data=[go.Surface(z=temp,x=RAIO,y=Z)])\n",
        "    # fig.update_layout(title=f\" {t} segundos\", autosize=False,\n",
        "    #               width=400, height=400)\n",
        "    # fig.show()\n",
        "\n",
        "# for KNBV in [0.0005,0.05,0.5]:\n",
        "#     for tempo in [0.1,0.6,1,1.5]:\n",
        "#         plot_3D_K(tempo, [KNBV])\n",
        "# for k,i in enumerate(np.linspace(0.05,2,80)):\n",
        "#     plot_3D_K(i, [0.5], k)\n",
        "plot_3D_K(1.5, [0.05])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r fotos_3_pe1_br0.zip saved_images_3"
      ],
      "metadata": {
        "id": "wwHomL8Mh9OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_T_R_Z_Error(Z, t = 1, kn_bv = 0.0002, resolution = 200):\n",
        "\n",
        "    R = torch.linspace(0, 1, resolution, dtype=torch.float32).reshape(resolution,1)\n",
        "    time = torch.ones(resolution, 1, dtype=torch.float32)*t\n",
        "    Kn_Bv = torch.ones(resolution, 1, dtype=torch.float32)*kn_bv\n",
        "    \n",
        "    T = torch.tensor(()).to(device)\n",
        "    Error = torch.tensor(()).to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    for i in range(len(Z)):\n",
        "        data = torch.hstack((R, torch.ones(resolution, 1, dtype=torch.float32)*Z[i], time, Kn_Bv)).to(device)\n",
        "        data.requires_grad_()\n",
        "        T = torch.hstack((T, model(data)))\n",
        "        Error = torch.hstack((Error, PDE_loss(model(data),data).reshape(resolution,1)))\n",
        "        \n",
        "\n",
        "    T = torch.t(T).cpu()\n",
        "    T = T.detach().numpy()\n",
        "    T = symmetric_data(T)\n",
        "    \n",
        "    Error = torch.t(Error).cpu()\n",
        "    Error = Error.detach().numpy()\n",
        "    Error = symmetric_data(Error)\n",
        "    \n",
        "    R = column_to_row(R.cpu().detach().numpy())\n",
        "    R = symmetric_data_1D(R)\n",
        "    \n",
        "    conso_data = np.array([T,Error])\n",
        "\n",
        "    fig, ax = plt.subplots(2, 1,figsize=(5, 5))\n",
        "    for a in range(2):\n",
        "        for i in range(len(Z)):\n",
        "            ax[a].plot(R, conso_data[a,i], label=f'K = {Z[i]}')\n",
        "        if a == 0:\n",
        "            # ax[a].set_ylim(None,1.1)\n",
        "            ax[a].set_xlabel('raio')  # Add an x-label to the axes.\n",
        "            ax[a].set_ylabel('Temperatura')  # Add a y-label to the axes.\n",
        "            ax[a].set_title(\"Gráfico Temperatura\")  # Add a title to the axes.\n",
        "            ax[a].legend();  # Add a legend.\n",
        "            ax[a].grid(True,alpha = 0.25)\n",
        "        else:\n",
        "            # ax[a].set_ylim(None,1.1)\n",
        "            ax[a].set_xlabel('raio')  # Add an x-label to the axes.\n",
        "            ax[a].set_ylabel('erro local')  # Add a y-label to the axes.\n",
        "            ax[a].set_title(\"Gráfico erro\")  # Add a title to the axes.\n",
        "            ax[a].legend();  # Add a legend.\n",
        "            ax[a].grid(True,alpha = 0.25)\n",
        "\n",
        "    # fig.savefig(\"aaa.svg\", format=\"svg\", dpi=1200)\n",
        "    \n",
        "plot_T_R_Z_Error([0.11,0.4,0.8,2], t = 0.8, kn_bv = 0.001, resolution = 100)"
      ],
      "metadata": {
        "id": "m5AFQoohwGbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_T_R_Z(Z, t, kn_bv = 0.0002, resolution = 200):\n",
        "\n",
        "    R = torch.linspace(0, 1, resolution, dtype=torch.float32).reshape(resolution,1)\n",
        "    Z_ = torch.ones(resolution, 1, dtype=torch.float32)\n",
        "    time = torch.ones(resolution, 1, dtype=torch.float32)\n",
        "    Kn_Bv = torch.ones(resolution, 1, dtype=torch.float32)*kn_bv\n",
        "    \n",
        "    conso_data = np.zeros((len(t), len(Z), resolution*2))\n",
        "    \n",
        "    model.eval()\n",
        "    for iii in range(len(t)):\n",
        "        T = torch.tensor(()).to(device)\n",
        "        for i in Z:\n",
        "            data = torch.hstack((R, Z_*i, time*t[iii], Kn_Bv)).to(device)\n",
        "            data.requires_grad_()\n",
        "            T = torch.hstack((T, model(data)))\n",
        "\n",
        "        T = torch.t(T).cpu()\n",
        "        T = T.detach().numpy()\n",
        "        T = symmetric_data(T)\n",
        "        conso_data[iii] = T\n",
        "    \n",
        "    R = column_to_row(R.cpu().detach().numpy())\n",
        "    R = symmetric_data_1D(R)\n",
        "\n",
        "    fig, ax = plt.subplots(len(t), 1,figsize=(5, 2.1*len(t)), layout='constrained')\n",
        "    for a in range(len(conso_data)):\n",
        "        for i in range(len(Z)):\n",
        "            ax[a].plot(R, conso_data[a,i], label=f'Z = {Z[i]}')\n",
        "        ax[a].set_ylim(None,1.2)\n",
        "        ax[a].set_xlabel('raio')  # Add an x-label to the axes.\n",
        "        ax[a].set_ylabel('Temperatura')  # Add a y-label to the axes.\n",
        "        if a == 0:\n",
        "            ax[a].set_title(f\"Gráfico Temperatura da seção \\n \\n Kn Bv = {2*kn_bv} e Br = 0 \\n \\n tempo = {t[a]}\")\n",
        "        else:\n",
        "            ax[a].set_title(f\"tempo = {t[a]}\")\n",
        "        if a == 0:\n",
        "            ax[a].legend();  # Add a legend.\n",
        "        ax[a].grid(True,alpha = 0.25)\n",
        "\n",
        "    fig.savefig(f\"saved_images_2D/plot_T_R_Z Kn_Bv={2*kn_bv},Br=0.svg\", format=\"svg\", dpi=300)\n",
        "\n",
        "for KNBV in [0.0005,0.05,0.5]:\n",
        "    plot_T_R_Z([0.15,0.4,0.8,4], t = [0.1,0.6,1,1.5], kn_bv = KNBV, resolution = 100)\n"
      ],
      "metadata": {
        "id": "Do34CNwtwGXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_Tmean_Z_time_Nu(Z_ = 18, t = [0.1,0.4,0.8,5], kn_bv = 0.01, resolution = 200, sampling = 1000):\n",
        "\n",
        "    Z = torch.linspace(0, Z_, resolution, dtype=torch.float32).reshape(resolution,1)\n",
        "    R = torch.from_numpy(chaospy.J(chaospy.Uniform(0, 1)).sample(sampling, rule=\"latin_hypercube\")).reshape(sampling,1)\n",
        "    R_Nu = torch.ones(resolution, 1, dtype=torch.float32).requires_grad_()\n",
        "    time = torch.ones(sampling, 1, dtype=torch.float32)\n",
        "    Kn_Bv = torch.ones(sampling, 1, dtype=torch.float32)*kn_bv\n",
        "    \n",
        "    T = torch.ones(len(t), resolution, dtype=torch.float32)\n",
        "    Nu = torch.zeros(len(t), resolution, dtype=torch.float32).to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    for i in range(len(t)):\n",
        "        for ii in range(resolution):\n",
        "            data = torch.hstack((R, torch.ones(sampling, 1)*Z[ii], time*t[i], Kn_Bv)).to(device, dtype=torch.float32)\n",
        "            T[i,ii] = T[i,ii]*(torch.mean(model(data)).item())\n",
        "            \n",
        "    for i in range(len(t)):\n",
        "        data_Nu = torch.hstack((R_Nu*0.9, Z, time[:resolution]*t[i], Kn_Bv[:resolution])).to(device, dtype=torch.float32).requires_grad_()\n",
        "        T_r = (torch.autograd.grad(model(data_Nu).sum(),data_Nu,create_graph=False,retain_graph=True)[0])[:,0]\n",
        "        Nu[i] = T_r\n",
        "        \n",
        "    Nu = -Nu/(1-T)\n",
        "    Nu = Nu.detach().numpy()\n",
        "    T = T.detach().numpy()\n",
        "        \n",
        "    T_Nu = np.array([T,Nu])\n",
        "        \n",
        "    Z = column_to_row(Z.numpy())\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 2,figsize=(10, 2.5), layout='constrained')\n",
        "    for a in range(2):\n",
        "        for i in range(len(t)):\n",
        "            ax[a].plot(Z, T_Nu[a,i], label=f'tempo = {t[i]}')\n",
        "        if a == 0:\n",
        "            # ax[a].set_ylim(None,1.1)\n",
        "            ax[a].set_xlabel('Comprimento')  # Add an x-label to the axes.\n",
        "            ax[a].set_ylabel('Temperatura média')  # Add a y-label to the axes.\n",
        "            ax[a].set_title(\"Temperatura média X Comprimento\")  # Add a title to the axes.\n",
        "            ax[a].legend();  # Add a legend.\n",
        "            ax[a].grid(True,alpha = 0.25)\n",
        "        else:\n",
        "            ax[a].set_ylim(-5,5)\n",
        "            ax[a].set_xlabel('Comprimento')  # Add an x-label to the axes.\n",
        "            ax[a].set_ylabel('Nusselt')  # Add a y-label to the axes.\n",
        "            ax[a].set_title(\"Nusselt X Comprimento\")  # Add a title to the axes.\n",
        "            ax[a].legend();  # Add a legend.\n",
        "            ax[a].grid(True,alpha = 0.25)\n",
        "\n",
        "    # fig.savefig(\"aaa.svg\", format=\"svg\", dpi=1200)\n",
        "    \n",
        "plot_Tmean_Z_time_Nu(Z_ = 1.5, kn_bv = 0.005)"
      ],
      "metadata": {
        "id": "uHIu5hV5wGTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_Tmean_Z_time(Z_ = 18, t = [0.1,0.4,0.8,5], kn_bv = 0.01, resolution = 200, sampling = 1000):\n",
        "\n",
        "    Z = torch.linspace(0, Z_, resolution, dtype=torch.float32).reshape(resolution,1)\n",
        "    R = torch.from_numpy(chaospy.J(chaospy.Uniform(0, 1)).sample(sampling, rule=\"latin_hypercube\")).reshape(sampling,1)\n",
        "    time = torch.ones(sampling, 1, dtype=torch.float32)\n",
        "    Kn_Bv = torch.ones(sampling, 1, dtype=torch.float32)*kn_bv\n",
        "    \n",
        "    T = torch.ones(len(t), resolution, dtype=torch.float32)\n",
        "    \n",
        "    model.eval()\n",
        "    for i in range(len(t)):\n",
        "        for ii in range(resolution):\n",
        "            data = torch.hstack((R, torch.ones(sampling, 1)*Z[ii], time*t[i], Kn_Bv)).to(device, dtype=torch.float32)\n",
        "            T[i,ii] = T[i,ii]*(torch.mean(model(data)).item())\n",
        "        \n",
        "    T = T.detach().numpy()\n",
        "        \n",
        "    Z = column_to_row(Z.numpy())\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(5, 2.7))\n",
        "    for i in range(len(t)):\n",
        "        ax.plot(Z, T[i])\n",
        "    # ax.set_ylim(None,1.1)\n",
        "    ax.set_xlabel('Comprimento')  # Add an x-label to the axes.\n",
        "    ax.set_ylabel('Temperatura média')  # Add a y-label to the axes.\n",
        "    ax.set_title(f\"Temperatura média X Comprimento \\n Kn Bv = {2*kn_bv} e Br = 0 \\n Figura: c)\")  # Add a title to the axes.\n",
        "    # ax.legend(loc = 'lower right');  # Add a legend.\n",
        "    ax.grid(True,alpha = 0.25)\n",
        "\n",
        "    fig.savefig(f\"saved_images/plot_Tmean-Kn Bn = {2*kn_bv} e Br = 0.svg\", format=\"svg\", dpi=300)\n",
        "\n",
        "# for KNBV in [0.0005,0.05,0.5]:\n",
        "#     plot_Tmean_Z_time(Z_ = 1.5, kn_bv = KNBV)\n",
        "plot_Tmean_Z_time(Z_ = 4.5, kn_bv = 0.5)"
      ],
      "metadata": {
        "id": "wdYJGy6pwGQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill between\n",
        "def temperatura_media(t_, kn_bv, Z_ = 1.5, resolution = 150, sampling = 800):\n",
        "\n",
        "    Z = torch.linspace(0, Z_, resolution, dtype=torch.float32).reshape(resolution,1)\n",
        "    R = torch.from_numpy(chaospy.J(chaospy.Uniform(0, 1)).sample(sampling, rule=\"latin_hypercube\")).reshape(sampling,1)\n",
        "    time = torch.ones(sampling, 1, dtype=torch.float32)*t_\n",
        "    Kn_Bv = torch.ones(sampling, 1, dtype=torch.float32)*kn_bv\n",
        "    \n",
        "    T = torch.ones(resolution, dtype=torch.float32)\n",
        "    \n",
        "    model.eval()\n",
        "    for i in range(resolution):\n",
        "        data = torch.hstack((R, torch.ones(sampling, 1)*Z[i], time, Kn_Bv)).to(device, dtype=torch.float32)\n",
        "        T[i] = T[i]*(torch.mean(model(data)).item())\n",
        "    \n",
        "    return T\n",
        "\n",
        "def plot_confiance_intervals(TMCA, Kn, t, Z_ = 1.5, resolution = 150, sampling = 1000):\n",
        "    \n",
        "    Z = torch.linspace(0, Z_, resolution, dtype=torch.float32).reshape(resolution,1)\n",
        "    Temp_geral = torch.zeros(len(TMCA), resolution, dtype=torch.float32).to(device)\n",
        "    T_statistics = torch.zeros(2, resolution, dtype=torch.float32)\n",
        "    \n",
        "    for et, k in enumerate(TMCA):\n",
        "        Temp_geral[et] = temperatura_media(t, k*Kn, Z_ = Z_, resolution = resolution, sampling = sampling)\n",
        "\n",
        "    for i in range(resolution):\n",
        "        T_statistics[0,i] = torch.mean(Temp_geral[:,i])\n",
        "        T_statistics[1,i] = torch.std(Temp_geral[:,i])\n",
        "\n",
        "    x = column_to_row(Z.numpy())\n",
        "    T_statistics = T_statistics.numpy()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, T_statistics[0], '-', label='1')\n",
        "    ax.fill_between(x, T_statistics[0] - 2*T_statistics[1], T_statistics[0] + 2*T_statistics[1], alpha=0.2)\n",
        "    ax.set_ylim(None,1.2)\n",
        "    ax.set_xlabel('comprimento')  # Add an x-label to the axes.\n",
        "    ax.set_ylabel('temperatura')  # Add a y-label to the axes.\n",
        "    ax.set_title(f\"Quantificação de incerteza para temperatura média \\n Kn = {Kn*2} tempo = {t} \\n 2 desvios-padrões de confiança\")  # Add a title to the axes.\n",
        "    # ax.legend();  # Add a legend.\n",
        "    ax.grid(True,alpha = 0.25)\n",
        "\n",
        "    fig.savefig(f\"incerteza - Kn={Kn} tempo={t}.svg\", format=\"svg\", dpi=300)\n",
        "\n",
        "for tempo in [0.2,0.5,1]:\n",
        "    for KN in [(0.1/2.0),(0.04/2.0),(0.008/2.0),(0.001/2.0)]:    \n",
        "        plot_confiance_intervals(beta_v, KN, tempo)"
      ],
      "metadata": {
        "id": "QZTda28lwGFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVxyi11F3i2H"
      },
      "outputs": [],
      "source": [
        "# ------------ export the NN to tensorboard ------------------\n",
        "exemple_data = torch.tensor([0.0,0.1,0.1]).to(device,dtype=torch.float32,).requires_grad_()\n",
        "writer.add_graph(model,exemple_data)\n",
        "\n",
        "# ------------ export the result image to tensorboard ------------------\n",
        "\n",
        "# writer.add_figure(\"Resultado\", fig)\n",
        "# writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-RRmk6i3i2I"
      },
      "outputs": [],
      "source": [
        "#---------matplotlib---------------- (2D)\n",
        "# plt.subplots(figsize=(10,10))\n",
        "# plt.scatter(*CC_1_CPU.T,facecolor='r',s=5)\n",
        "# plt.scatter(*CC_2_CPU.T,facecolor='r',s=5)\n",
        "# plt.scatter(*CC_3_CPU.T,facecolor='r',s=5)\n",
        "# plt.scatter(*adap_sam.T,s=5)\n",
        "# plt.show()\n",
        "\n",
        "#---------plotly----------------\n",
        "fig = go.Figure(data=[go.Scatter3d(x=adap_sam.T[0], y=adap_sam.T[1], z=adap_sam.T[2], mode='markers',\n",
        "                                   marker=dict(size=1))])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "DS_train = especial_sorting(dynamic_S, axis=2, engine=\"torch\")\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "print('Time: ', stop - start) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoQ4vha86uvP",
        "outputId": "818d011c-4845-43a8-b283-96555c1f0b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:  0.30932444199999054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testar_erro():\n",
        "    # teste dos erros \n",
        "    def Adap_MSELoss(X):\n",
        "        return torch.mean(torch.pow(X,2))\n",
        "    \n",
        "    loss_fn = Adap_MSELoss\n",
        "    \n",
        "    # ------------------------------------------\n",
        "    \n",
        "    problem_domain = [0,1,0,12,0,10,0.0002,1,0.0001,0.1]\n",
        "    \n",
        "    uniform_samples = torch.from_numpy(data_sampling(problem_domain, 100000, rule=\"latin_hypercube\" ).T)\n",
        "    CC_1_test = torch.from_numpy(smap(lambda t: t*0,40000,problem_domain,\"x\",\"latin_hypercube\").T)\n",
        "    CC_2_test = torch.from_numpy(smap(lambda t: t*0+1,40000,problem_domain,\"x\",\"latin_hypercube\").T)\n",
        "    CC_3_test = torch.from_numpy(smap(lambda t: t*0,40000,problem_domain,\"y\",\"latin_hypercube\").T)\n",
        "    CC_4_test = torch.from_numpy(smap(lambda t: t*0+12,40000,problem_domain,\"y\",\"latin_hypercube\").T)\n",
        "    CC_5_test = torch.from_numpy(smap(lambda x: x*0,40000,problem_domain,\"t\",\"latin_hypercube\").T)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    X = uniform_samples.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_1 = CC_1_test.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_2 = CC_2_test.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_3 = CC_3_test.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_4 = CC_4_test.to(device,dtype=torch.float32).requires_grad_()\n",
        "    CC_5 = CC_5_test.to(device,dtype=torch.float32).requires_grad_()\n",
        "       \n",
        "    pred_domain = PDE_loss(model(X),X)\n",
        "    pred_boundery_1 = BC_1(model(CC_1),CC_1)\n",
        "    pred_boundery_2 = BC_2(model(CC_2),CC_2)\n",
        "    pred_boundery_3 = BC_3(model(CC_3))\n",
        "    pred_boundery_4 = BC_4(model(CC_4),CC_4)\n",
        "    pred_boundery_5 = BC_5(model(CC_5))\n",
        "        \n",
        "    domain_test = loss_fn(pred_domain)\n",
        "    boundery_test_1 = loss_fn(pred_boundery_1)\n",
        "    boundery_test_2 = loss_fn(pred_boundery_2)\n",
        "    boundery_test_3 = loss_fn(pred_boundery_3)\n",
        "    boundery_test_4 = loss_fn(pred_boundery_4)\n",
        "    boundery_test_5 = loss_fn(pred_boundery_5)\n",
        "    \n",
        "    print(f\" testes \\n domain_test: {domain_test} \\n boundery_test_1: {boundery_test_1} \\n boundery_test_2: {boundery_test_2} \\n boundery_test_3: {boundery_test_3} \\n boundery_test_4: {boundery_test_4} \\n boundery_test_5: {boundery_test_5}\")\n",
        "\n",
        "testar_erro()"
      ],
      "metadata": {
        "id": "waWin-jMQ6TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r fotos.zip saved_images"
      ],
      "metadata": {
        "id": "9q8dMS6ztg18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir saved_images_1\n",
        "!mkdir saved_images_2\n",
        "!mkdir saved_images_3"
      ],
      "metadata": {
        "id": "cyprFI6HiPFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}